# -*- coding: utf-8 -*-
"""exchangeAnalysis.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1iRZZ5Aua6uBezp1fcUDCWGOHzZqejbHf
"""

import pandas as pd 
import numpy as np 
import seaborn as sns
import matplotlib.pyplot as plt

from google.colab import drive
drive.mount('/content/gdrive')

# allow full display of DataFrame rows 
pd.set_option('display.max_rows', None) 

# config fig size
sns.set(rc={'figure.figsize':(14.0,6.0)})

root_path = 'gdrive/My Drive/AirQualityUCI.csv'

# read json file 
df = pd.read_json(root_path, lines=True)
df.head()

"""Json data structure is nested and requires further expansion of 'app_data. Given the size of the file I'll split it by name first"""

# check different names
df.name.unique()

# split data by name 
exchange = df[df.name == 'tc3.core.exchange']
trader = df[df.name == 'tc3.core.trader']
brokers = df[df.name == 'tc3.execution.brokers']

"""# **Exchange**"""

exchange.head()

"""First two rows are descriptive and contain info about racing horses. 
Subsequent rows contain data timeseries

### **Exchange** **descriptive**
"""

# exctract exchange descriptive rows 
exchange_desc = pd.json_normalize(exchange.to_dict(orient='record'),record_path=["app_data", "runners"],meta=['time'])

# select only elements of first row as second is duplicate
exchange_desc = pd.DataFrame(exchange_desc).iloc[:19]

# remove undescriptive columns 
cols =['status','sp','removalDate','matches','totalMatched','adjustmentFactor','orders',
       'lastPriceTraded','ex.availableToBack','ex.availableToLay','ex.tradedVolume','metadata.OFFICIAL_RATING']
exchange_desc.drop(cols, inplace=True, axis=1)
exchange_desc

# number of participants  
num_part = exchange_desc.selectionId.nunique()
print('There are',num_part,'participants at the race')

print('Participants name list: \n', exchange_desc.runnerName.unique())

selectionId = exchange_desc.selectionId
print('Participants identification Id \n', selectionId)

"""### **Implied** **Probability**"""

# extract implied probability from forecast price  
implied_prob = exchange_desc[['selectionId','metadata.runnerId','runnerName','metadata.FORECASTPRICE_NUMERATOR','metadata.FORECASTPRICE_DENOMINATOR']]
implied_prob['impliedProb %'] = round(exchange_desc['metadata.FORECASTPRICE_DENOMINATOR'].astype(float) / (exchange_desc['metadata.FORECASTPRICE_NUMERATOR'].astype(float) + exchange_desc['metadata.FORECASTPRICE_DENOMINATOR'].astype(float)),4) * 100
implied_prob

# market overround is 6.06%
overround = implied_prob['impliedProb %'].sum()
print('Market overround is', overround)

"""### **Exchange** **time** **series** **data**"""

# select data after descriptive rows
data = exchange.iloc[38:].to_dict(orient='record')

# flat first level json keys
exchange_first = pd.json_normalize(data,record_path=["app_data", "runners"],meta=['time'])
exchange_first = pd.DataFrame(exchange_first)
exchange_first.head()

"""Columns 'matches' , 'availableToBack' , 'availableToLay' are further nested within 'runners' key. Will expand them later as required

**Market** **analysis**
"""

# expand 'matches' path
exchange_trades = pd.json_normalize(data,record_path=["app_data",'runners','matches'],meta=[['app_data','runners','lastPriceTraded'],['app_data','runners','totalMatched'],
                                                                                             ['app_data','lastMatchTime'],['time'],['app_data','runners','selectionId']])

# cols to drop 
cols = ['matchDate','matchId','betId']
exchange_trades.drop(cols, inplace=True, axis=1)
exchange_trades.sort_values('size').tail()

time_start = exchange_trades['time'].iloc[:1]
print("Log time start is {}" .format(time_start))

time_end = exchange_trades['time'].iloc[-1]
print("Log time end is {}" .format(time_end))

traded_names = exchange_trades['app_data.runners.selectionId'].nunique()
print('Number of traded names in this timeframe is {}' .format(traded_names))

num_orders = exchange_trades.shape[0]
print('Number of orders sent in this timeframe is {}'.format(num_orders))

tot_exec = exchange_trades['size'].sum()
print('Notional of orders sent in this timeframe is {}'.format(tot_exec))

mean_size = exchange_trades['size'].mean()
print('Average order size is {}'.format(round((mean_size),2)))

exchange_trades['size'].hist()
plt.title('Order size distribution')
plt.xlabel('Order size amount')
plt.ylabel('Number of orders')

"""Average order size is  371.98. Order size is not normally distributed with long left tail. Far end of the curve is populatd by large lay orders of size 	2175.89 on selectionId  '25013915'"""

exec_size_std = exchange_trades['size'].std()
print('Order size standard deviation is', round((exec_size_std),2))

# total traded volume by name 
selectionId_traded = [27157433, 25013915, 24044533, 23225592, 26986439, 28095568, 28247531,28232003, 27915468, 24707404, 24245268]

for runner in selectionId_traded:
  tot_matched = exchange_trades[exchange_trades['app_data.runners.selectionId'] == runner]['app_data.runners.totalMatched'].iloc[-1]
  print('Selection ID {} total traded volume is {}' .format(runner, tot_matched))

"""Selection ID 27157433 traded approx 440k which is 3x more than the second. Below plot illustrates the difference in volume traded by race participants. Volume is > 0 at the start because matches occured prior to logs start"""

for runner in selectionId_traded:
  runner_vol = exchange_trades[exchange_trades['app_data.runners.selectionId'] == runner]['app_data.runners.totalMatched'].plot()
  plt.title('Total traded volume by race participants')
  plt.xlabel('Iteration')
  plt.ylabel('Volume traded')
  plt.legend(exchange_desc['runnerName'])

"""**Order** **books** **liquidity**

Below are the order books of top 3 traded names

**Farmix**
"""

# extract lay timeseries price data
exchange_lay = pd.json_normalize(data,record_path=["app_data", "runners","ex","availableToLay"],meta=[['time'],['app_data','runners','selectionId']])
exchange_lay = pd.DataFrame(exchange_lay)

# get farmix lay side of order book at the start of the period 
farmix_lay = exchange_lay[exchange_lay['app_data.runners.selectionId'] == 27157433].set_index('app_data.runners.selectionId')
farmix_lay = farmix_lay.rename(columns={"price": "lay", "size": "lay size","app_data.runners.selectionId":"selectionId"}).head(3)[['lay','lay size']]

# extract back timeseries price data
exchange_back = pd.json_normalize(data,record_path=["app_data", "runners","ex","availableToBack"],meta=[['time'],['app_data','runners','selectionId']])
exchange_back = pd.DataFrame(exchange_back)

# get farmix back side of order book at the start of the period 
fermix_back = exchange_back[exchange_back['app_data.runners.selectionId'] == 27157433].set_index('app_data.runners.selectionId')
farmix_back = fermix_back.rename(columns={"price": "back", "size": "back size","app_data.runners.selectionId":"selectionId"}).head(3)[['back size','back']]

# concat farmix lay and back 
farmix_book = pd.concat([farmix_back, farmix_lay], axis=1)
farmix_book

farmix_spread = (farmix_book['lay'].iloc[0] - farmix_book['back'].iloc[0])
farmix_liq_back = farmix_book['back size'].sum()
farmix_liq_lay = farmix_book['lay size'].sum()

print('Farmix order book spread in {:.2f} ticks wide'.format(farmix_spread))
print('Farmix BACK liquidity at touch is {}'.format(farmix_book['back size'].iloc[0]))
print('Farmix LAY liquidity at touch is {}'.format(farmix_book['lay size'].iloc[0]))
print('Farmix BACK liquidity across first three price lavels is {}'.format(farmix_liq_back))
print('Farmix BACK liquidity across first three price lavels is {}'.format(farmix_liq_lay))

"""**Florrie** **Webb**"""

# get florrie lay side of order book at the start of the period 
florrie_lay = exchange_lay[exchange_lay['app_data.runners.selectionId'] == 25013915].set_index('app_data.runners.selectionId')
florrie_lay = florrie_lay.rename(columns={"price": "lay", "size": "lay size","app_data.runners.selectionId":"selectionId"}).head(3)[['lay','lay size']]

# get florrie back side of order book at the start of the period 
florrie_back = exchange_back[exchange_back['app_data.runners.selectionId'] == 25013915].set_index('app_data.runners.selectionId')
florrie_back = florrie_back.rename(columns={"price": "back", "size": "back size","app_data.runners.selectionId":"selectionId"}).head(3)[['back size','back']]

# concat florrie lay and back 
florrie_book = pd.concat([florrie_back, florrie_lay], axis=1)
florrie_book

florrie_spread = (florrie_book['lay'].iloc[0] - florrie_book['back'].iloc[0])
florrie_liq_back = florrie_book['back size'].sum()
florrie_liq_lay = florrie_book['lay size'].sum()

print('Florrie order book spread in {:.2f} ticks wide'.format(florrie_spread))
print('Florrie BACK liquidity at touch is {}'.format(florrie_book['back size'].iloc[0]))
print('Florrie LAY liquidity at touch is {}'.format(florrie_book['lay size'].iloc[0]))
print('Florrie BACK liquidity across first three price lavels is {}'.format(florrie_liq_back))
print('Florrie LAY liquidity across first three price lavels is {:2f}'.format(florrie_liq_lay))

"""**Coolagh** **Park**"""

# get coolagh lay side of order book at the start of the period 
coolagh_lay = exchange_lay[exchange_lay['app_data.runners.selectionId'] == 24044533].set_index('app_data.runners.selectionId')
coolagh_lay = coolagh_lay.rename(columns={"price": "lay", "size": "lay size","app_data.runners.selectionId":"selectionId"}).head(3)[['lay','lay size']]

# get coolagh back side of order book at the start of the period 
coolagh_back = exchange_back[exchange_back['app_data.runners.selectionId'] == 24044533].set_index('app_data.runners.selectionId')
coolagh_back = coolagh_back.rename(columns={"price": "back", "size": "back size","app_data.runners.selectionId":"selectionId"}).head(3)[['back size','back']]

# concat coolagh lay and back 
coolagh_book = pd.concat([coolagh_back, coolagh_lay], axis=1)
coolagh_book

coolagh_spread = (coolagh_book['lay'].iloc[0] - coolagh_book['back'].iloc[0])
coolagh_liq_back = coolagh_book['back size'].sum()
coolagh_liq_lay = coolagh_book['lay size'].sum()

print('Coolagh order book spread in {:.2f} ticks wide'.format(coolagh_spread))
print('Coolagh BACK liquidity at touch is {}'.format(coolagh_book['back size'].iloc[0]))
print('Coolagh LAY liquidity at touch is {}'.format(coolagh_book['lay size'].iloc[0]))
print('Coolagh BACK liquidity across first three price lavels is {:2f}'.format(coolagh_liq_back))
print('Coolagh LAY liquidity across first three price lavels is {:2f}'.format(coolagh_liq_lay))

# exctract orders and fills 
brokers_df = pd.json_normalize(brokers.to_dict(orient='record'),record_path=[['app_data','instructionReports']])
brokers_df.tail()

# list of order status 
brokers_df.orderStatus.unique()

# number of orders
tot_orders = brokers_df[brokers_df['orderStatus'] == 'EXECUTABLE'].shape[0]
print('Number of executable orders sent out is {}'.format(tot_orders))

# number of cancelled orders 
canc_orders = brokers_df[brokers_df['sizeCancelled'] > 0.001]
print('Number of cancelled orders is {}'. format(canc_orders.shape[0]))

# total exec 
tot_fills = brokers_df[brokers_df['orderStatus'] == 'EXECUTION_COMPLETE'].shape[0]
print('Number of executed orders is {}'.format(tot_fills))

filled_names = brokers_df[brokers_df['orderStatus'] == 'EXECUTION_COMPLETE']['instruction.selectionId'].nunique()
print('These {} orders were executed across {} different names'.format(tot_fills,filled_names))

# names orders sent  
num_traded = brokers_df['instruction.selectionId'].nunique()
not_filled = num_traded - filled_names
print('Orders were sent across {} different names' .format(num_traded))
print('Therefore on {} names orders were not filled'.format(not_filled))

# order fill ratio percentage 
orderFill = round((tot_fills / tot_orders),4) * 100
print('Order fill ratio is {} %' .format(orderFill))

# tot size orders sent out 
order_size = brokers_df['instruction.limitOrder.size'].sum()
print('Total size of orders sent out is {:2f}' .format(order_size))

"""**Fill** **analysis**"""

# get executed orders 
fills = brokers_df[brokers_df['orderStatus'] == 'EXECUTION_COMPLETE']
fills.head()

"""**Back** **fills**"""

# number of back fills 
back_fills = brokers_df[(brokers_df['orderStatus'] == 'EXECUTION_COMPLETE') & (brokers_df['instruction.side'] == 'BACK')]
print('Number of back fills is {}'.format(back_fills.shape[0]))

# total back size matched 
back_matched = round((back_fills['sizeMatched'].sum()),4)
print('Total back size matched is {}'.format(back_matched))

# average back price 
back_fills['price_size'] = (back_fills['averagePriceMatched'] * back_fills['sizeMatched'])
avg_back_price = round((back_fills['price_size'].sum() / back_fills['sizeMatched'].sum()),4)

print("Weighted average back price is {:2f}".format(avg_back_price))

# back exposure 
back_exposure = round((back_matched * avg_back_price),2)
print('Back exposure on this race is {}'.format(back_exposure))

"""**Lay** **fills**"""

# number of lay fills 
lay_fills = brokers_df[(brokers_df['orderStatus'] == 'EXECUTION_COMPLETE') & (brokers_df['instruction.side'] == 'LAY')]
print('Number of back fills is {}'.format(lay_fills.shape[0]))

# total lay size matched 
lay_matched = round((lay_fills['sizeMatched'].sum()),4)
print('Total back size matched is {}'.format(lay_matched))

# average lay price 
lay_fills['price_size'] = (lay_fills['averagePriceMatched'] * lay_fills['sizeMatched'])
avg_lay_price = round((lay_fills['price_size'].sum() / lay_fills['sizeMatched'].sum()),4)

print("Weighted average back price is {}".format(avg_back_price))

# lay exposure
lay_exposure = round((lay_matched * avg_lay_price),2)
print('Lay exposure on this race is {}'.format(lay_exposure))

"""### **Observations**

From what I could see in the logs liquidity was provided on Betfair exchange layering orders across three levels of the book on any traded name. 

As new data comes is these orders are then cancelled, size and price adjusted and replaced.

Style of trading appears not to be directional but rather delta neutral with back and lay exposure of very similar size

### **Evaluate** **trading** **efficiency**
"""

# get all fills 
fills = brokers_df[(brokers_df['orderStatus'] == 'EXECUTION_COMPLETE')]
fills['price_size'] = fills['sizeMatched'] * fills['averagePriceMatched']
fills.head()

# traded Ids  
tradedIds = fills['instruction.selectionId'].unique()

# function to get final expouse, side and avg price on all traded names 
for id in tradedIds:
  position = brokers_df[(brokers_df['orderStatus'] == 'EXECUTION_COMPLETE') & (brokers_df['instruction.selectionId'] == id)]['sizeMatched'].sum()
  price_size = fills[(fills['orderStatus'] == 'EXECUTION_COMPLETE') & (fills['instruction.selectionId'] == id)]['price_size'].sum()
  price_sum = fills[(fills['orderStatus'] == 'EXECUTION_COMPLETE') & (fills['instruction.selectionId'] == id)]['averagePriceMatched'].sum()
  size_sum = fills[(fills['orderStatus'] == 'EXECUTION_COMPLETE') & (fills['instruction.selectionId'] == id)]['sizeMatched'].sum()
  weighted_average_price = (price_size / size_sum)
  side = brokers_df[(brokers_df['orderStatus'] == 'EXECUTION_COMPLETE') & (brokers_df['instruction.selectionId'] == id)]['instruction.side'].iloc[-1]

  print('Position on Id {} is {} {} at weighted average price of {}'.format(int((id)), side, round((position),2),round((weighted_average_price),2)))

# map Ids to runner names 
exchange_desc[['runnerName','selectionId']]

"""Race exposure is the following: 

**Jack Hackett** *backed* 1562.85 at weighted average price of 10.61

**Rebel Gold** *backed* 74.17 at weighted average price of 11.05

**Florrie Webb** *laid* 27.74 at weighted average price of 4.96

**Giftedtokate** *backed* 57.33 at weighted average price of 132.66

**She Tops The Lot** *laid* 66.05 at weighted average price of 107.7

**Satin Sun** *laid* 467.48 at weighted average price of 39.77
"""

for id in tradedIds:
  position = brokers_df[(brokers_df['orderStatus'] == 'EXECUTION_COMPLETE') & (brokers_df['instruction.selectionId'] == id)]['sizeMatched'].sum()
  price_size = fills[(fills['orderStatus'] == 'EXECUTION_COMPLETE') & (fills['instruction.selectionId'] == id)]['price_size'].sum()
  price_sum = fills[(fills['orderStatus'] == 'EXECUTION_COMPLETE') & (fills['instruction.selectionId'] == id)]['averagePriceMatched'].sum()
  size_sum = fills[(fills['orderStatus'] == 'EXECUTION_COMPLETE') & (fills['instruction.selectionId'] == id)]['sizeMatched'].sum()
  weighted_average_price = (price_size / size_sum)
  exposure = position * weighted_average_price
  side = brokers_df[(brokers_df['orderStatus'] == 'EXECUTION_COMPLETE') & (brokers_df['instruction.selectionId'] == id)]['instruction.side'].iloc[-1]

  print('{} position of {} at price of {} on ID {} gives total exposure of {}'.format(side ,round((position),2), round((weighted_average_price),2), int(id), round((exposure),2)))

"""Out of the six positions, three were backed and three were laid. Note how backed positions have corresponding laid positions, similar to a long short equity portfolio. Clear example of this is ID 23225592 with ID 28095568

Total back exposure is 25010.2 whereas lay exposure is 25836.86

Outcome of the race was: 

https://www.sportinglife.com/racing/results/2020-03-24/clonmel/569131/money-back-on-the-boylesports-app-maiden-hurdle

1st Farmix 

2nd Rebel Gold

3rd Jack Hackett

4th Satin Sun

5th Boghlone Honey

6th She Tops The Lot

7th Morning Glisten

8th Cornhill Lass

9th Gone Racing

10th Florrie Webb

11th Carriealice

12th Coolagh Park

13th Giftedtokate

14th Kerrymaygold

Farmix won the race. There was not position on this name. Therefore, assuming no other orders were executed prior to the start of these logs, the outcome of the race was to 'collect premius' from laying IDs 25013915, 28247531, 28095568

### **Plan** **for** **improving** **execution**

Most orders were executed 30 seconds before race start or few seconds in-play.

Taking selectionId '28232003' as an example, all four orders were executed in the space of 9 seconds. Price impact was significant where first back fill was at 140 and last one at 110
"""

orders = brokers_df[(brokers_df['orderStatus'] == 'EXECUTION_COMPLETE') & (brokers_df['instruction.selectionId'] == 28232003.0)][['orderStatus','placedDate','instruction.orderType','instruction.side',
                                                                            'instruction.orderType','instruction.limitOrder.price','instruction.limitOrder.size','instruction.limitOrder.persistenceType']]
orders

"""One way for improving execution would be to adopt VWAP or TWAP algorithms with features preventing orders from taking out levels on the book.

Performance can be analysed in a simulated environment where VWAP or TWAP algorithm price fills is compared to current execution methods
"""